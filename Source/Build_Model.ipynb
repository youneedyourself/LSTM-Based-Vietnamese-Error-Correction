{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0w3z3cEFZhQ1"},"outputs":[],"source":["import re\n","import numpy as np\n","import pickle\n","import os"]},{"cell_type":"markdown","metadata":{"id":"gV7gnnYnZMns"},"source":["#1.THU THẬP DỮ LIỆU"]},{"cell_type":"markdown","source":["#### Đọc dữ liệu từ folder."],"metadata":{"id":"fZfKuIjmHoHN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"znTxFbf0YjM0"},"outputs":[],"source":["class FileData(object):\n","    def __init__(self, path):\n","        self.path = path\n","        with open(path, encoding='utf-16') as f:\n","          self.data = f.read()\n","          #print(self.data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"62R5ekxBZoQR"},"outputs":[],"source":["ABSOLUTE_PATH = r\"E:\\Python\\language\\Train_Full\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-lyYbU9AR_AO"},"outputs":[],"source":["c_tri =  \"\\Chinh tri Xa hoi\"\n","\n","d_song = \"\\Doi song\"\n","\n","khoa_hoc = \"\\Khoa hoc\"\n","\n","kinh_doanh = \"\\Kinh doanh\"\n","\n","p_luat = \"\\Phap luat\"\n","\n","suc_khoe = \"\\Suc khoe\"\n","\n","the_gioi = \"\\The gioi\"\n","\n","the_thao = \"\\The thao\"\n","\n","van_hoa = \"\\Van hoa\"\n","\n","vi_tinh = \"\\Vi tinh\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OlevGeM6Cmuz"},"outputs":[],"source":["c_tri =  \"/Chinh tri Xa hoi\"\n","\n","#d_song = \"/Doi song\"\n","\n","khoa_hoc = \"/Khoa hoc\"\n","\n","kinh_doanh = \"/Kinh doanh\"\n","\n","p_luat = \"/Phap luat\"\n","\n","suc_khoe = \"/Suc khoe\"\n","\n","#the_gioi = \"/The gioi\"\n","\n","the_thao = \"/The thao\"\n","\n","#van_hoa = \"/Van hoa\"\n","\n","#vi_tinh = \"/Vi tinh\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ebuOhjqdR6A"},"outputs":[],"source":["#corpus = [c_tri, d_song, khoa_hoc, kinh_doanh, p_luat, suc_khoe, the_gioi, the_thao, van_hoa, vi_tinh]\n","\n","# Giam thieu du lieu\n","corpus = [c_tri, khoa_hoc, kinh_doanh, p_luat, suc_khoe, the_thao]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VdxyVSPJZ1eK"},"outputs":[],"source":["for folder_path in range(len(corpus)):\n","    corpus[folder_path] = ABSOLUTE_PATH + corpus[folder_path]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3w88ltME4SiP","outputId":"72ea8bcd-0bcc-45c6-8f9f-b7a94be6438c"},"outputs":[{"data":{"text/plain":["'E:\\\\Python\\\\language\\\\Train_Full/Kinh doanh'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["corpus[2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"La-aEcptdPgo"},"outputs":[],"source":["import os\n","file_list = []\n","#count = 0\n","\n","for folder_path in corpus:\n","    count = 0\n","    for name in os.listdir(folder_path):\n","        count +=1\n","        if count == 1500:\n","          break\n","        path = os.path.join(folder_path, name)\n","        if not os.path.isfile(path):\n","            continue\n","        file = FileData(path)\n","        file_list.append( file.data )\n","        # count +=1\n","        # if count == 5:\n","        #   break"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Bpdg-jn87df","outputId":"5a8fc0bd-dbd9-4651-b6a8-e769f8a7450d"},"outputs":[{"data":{"text/plain":["8994"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["len(file_list)"]},{"cell_type":"markdown","metadata":{"id":"3IArL8BUCmu1"},"source":["#### Lưu dữ liệu dưới dạng file pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzFpHKHaP0l2"},"outputs":[],"source":["#path_corpus = \"E:\\Python\\language\\input_corpus.pkl\"\n","path_corpus = 'E:\\Python\\language\\input_cutdown_corpus.pkl'\n","\n","with open(path_corpus, 'wb') as pickle_file:\n","    pickle.dump(file_list, pickle_file)"]},{"cell_type":"markdown","metadata":{"id":"GMAmC-ktKreu"},"source":["# 2.IMPORT Dữ liệu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mRph_nIndXi9"},"outputs":[],"source":["pk_path = '/home/phuong/Documents/2023_NLP/Code/PreData/input_cutdown_corpus.pkl'"]},{"cell_type":"markdown","source":["#### Load dữ liệu đã được lưu trữ ở phần 1 ra sử dụng"],"metadata":{"id":"mLZ1tdpcH6V5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Dr2HuIqQBIB"},"outputs":[],"source":["with open(pk_path, \"rb\") as f:\n","    data = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-jXs1dPCRcPd","outputId":"9ea0e934-b431-4127-cb19-cc3f2fd93a05"},"outputs":[{"data":{"text/plain":["8994"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["len(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YidyP3TqVxKu","outputId":"36f695be-a319-45ee-ceaa-f9eef73c7219"},"outputs":[{"data":{"text/plain":["' Xây dựng trạm dừng nghỉ cho xe đò\\nTrên diện tích 10.000m2 sẽ xây dựng khu nhà ăn tự chọn phục vụ 400-500 người, khu siêu thị, trạm y tế, điện thoại công cộng, hai khu vực nhà vệ sinh, trạm xăng dầu... (ảnh mô hình). Dự kiến tổng kinh phí đầu tư khoảng 5 tỉ đồng, công trình sẽ thi công và đưa vào sử dụng trong năm 2005. \\n\\n'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["data[1]"]},{"cell_type":"markdown","metadata":{"id":"hEXUdGdxKxw3"},"source":["### 2.1 TIEN XU LY"]},{"cell_type":"markdown","metadata":{"id":"unakwWmbLmrN"},"source":["#### Xây dựng bộ ngữ liệu là 1 list chứa các câu. Thay thế dấu cách xuống dòng bằng dấu chấm cuối câu. Từ dấu chấm đó, tách văn bản theo các câu."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HOhfO5RzV1GW"},"outputs":[],"source":["import re\n","alphabet = '^[ _abcdefghijklmnopqrstuvwxyz0123456789áàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđ!\\\"\\',\\-\\.:;?_\\(\\)]+$'\n","def latin_extract(data):\n","    latin_extract_data=[]\n","    # duyet qua tung van ban\n","    for i in data:\n","      if i == 1:\n","        break\n","      # thay the xuong dong la dau cham ket thuc\n","      i=i.replace(\"\\n\",\".\")\n","      # tach van ban theo dau cham ket thuc\n","      sentences=i.split(\".\")\n","      # re.match(alphabet, j.lower()): kiểm tra xem phiên bản viết thường j có trong alphabet hay không.\n","      for j in sentences:\n","        if len(j.split()) > 2 and re.match(alphabet, j.lower()):\n","            latin_extract_data.append(j)\n","\n","    return latin_extract_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7_YqN_DJYUA","outputId":"bf00b797-6c1b-4afe-aa14-d99d9141f58e"},"outputs":[{"name":"stdout","output_type":"stream","text":["137028\n"]},{"data":{"text/plain":["' Từ 2-12: cấp giấy chứng nhận quyền sử dụng đất theo mẫu mới'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["training_data = latin_extract(data)\n","\n","print(len(training_data))\n","training_data[10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sp6NYXjjPPm8","outputId":"ae5244d4-840d-417a-8801-97572dd6d460"},"outputs":[{"name":"stdout","output_type":"stream","text":["Đông đảo người dân đến vui chơi tại Thảo Cầm Viên - Ảnh: T\n","Các cổ động viên Bảo vệ thực vật Sài Gòn - Dofilm và nông nghiệp Sài Gòn Pakse đang chào đón cuộc đua - Ảnh: Nguyên Khôi \n","Lễ mít tinh kỷ niệm 31 năm ngày giải phóng miền Nam, thống nhất đất nước, ngày Quốc tế Lao động 1-5 và chào mừng Đại hội X của Đảng thành công đã được tổ chức long trọng vào sáng 27-4\n","Ngày 28-4, Chương trình nghệ thuật đặc biệt \"Cảm xúc tháng tư\" do Sở VH-TT TP\n","HCM phối hợp với Đài truyền hình VN tổ chức tại công viên 30-4 thu hút đông đảo người dân đến xem\n"," Anh Nguyễn Hồng (Quận Bình Thạnh) chia sẻ \"3 năm liền đều ra đây xem Cúp truyền hình rồi, chỉ để xem được mấy giây tay đua nào cán đích nhưng tôi đem sẵn cả nước, báo và ổ bánh mì ra đây\"\n","Giữa dòng người, hai bạn trẻ Thùy, Hạnh vẫn thoăn thoắt giới thiệu sản phẩm và bán nước Trà xanh không độ\n"," \"Hôm nay không khí đông vui quá, hàng bán chạy hẳn, ráng xong ngày hôm nay thôi, nhóm mình sẽ đi chơi vào ngày mai\"\n","Peter và Anna đến từ Đan Mạch không ngừng chụp hình không khí tại công viên\n"," Anna đang làm việc tại Việt Nam, còn Peter chỉ là khách du lịch bị Anna \"dụ dỗ\" sang VN chơi lễ\n"]}],"source":["# Hien thi de check\n","\n","i = 100\n","while i < 110:\n","  print(training_data[i])\n","  i += 1"]},{"cell_type":"markdown","metadata":{"id":"kl9u9zEfCmu2"},"source":["#### Liệt kê các từ, các kí tự gõ sai thường gặp trong tiếng Việt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nw2dp5NzJpp5"},"outputs":[],"source":["import numpy as np\n","from unidecode import unidecode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6t2kr8pai6l"},"outputs":[],"source":["letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n","letters2=list(\"abcdefghijklmnopqrstuvwxyz\")\n","\n","typo={\"ă\":\"aw\",\"â\":\"aa\",\"á\":\"as\",\"à\":\"af\",\"ả\":\"ar\",\"ã\":\"ax\",\"ạ\":\"aj\",\"ắ\":\"aws\",\"ổ\":\"oor\",\"ỗ\":\"oox\",\"ộ\":\"ooj\",\"ơ\":\"ow\",\n","\"ằ\":\"awf\",\"ẳ\":\"awr\",\"ẵ\":\"awx\",\"ặ\":\"awj\",\"ó\":\"os\",\"ò\":\"of\",\"ỏ\":\"or\",\"õ\":\"ox\",\"ọ\":\"oj\",\"ô\":\"oo\",\"ố\":\"oos\",\"ồ\":\"oof\",\n","\"ớ\":\"ows\",\"ờ\":\"owf\",\"ở\":\"owr\",\"ỡ\":\"owx\",\"ợ\":\"owj\",\"é\":\"es\",\"è\":\"ef\",\"ẻ\":\"er\",\"ẽ\":\"ex\",\"ẹ\":\"ej\",\"ê\":\"ee\",\"ế\":\"ees\",\"ề\":\"eef\",\n","\"ể\":\"eer\",\"ễ\":\"eex\",\"ệ\":\"eej\",\"ú\":\"us\",\"ù\":\"uf\",\"ủ\":\"ur\",\"ũ\":\"ux\",\"ụ\":\"uj\",\"ư\":\"uw\",\"ứ\":\"uws\",\"ừ\":\"uwf\",\"ử\":\"uwr\",\"ữ\":\"uwx\",\n","\"ự\":\"uwj\",\"í\":\"is\",\"ì\":\"if\",\"ỉ\":\"ir\",\"ị\":\"ij\",\"ĩ\":\"ix\",\"ý\":\"ys\",\"ỳ\":\"yf\",\"ỷ\":\"yr\",\"ỵ\":\"yj\",\"đ\":\"dd\",\n","\"Ă\":\"Aw\",\"Â\":\"Aa\",\"Á\":\"As\",\"À\":\"Af\",\"Ả\":\"Ar\",\"Ã\":\"Ax\",\"Ạ\":\"Aj\",\"Ắ\":\"Aws\",\"Ổ\":\"Oor\",\"Ỗ\":\"Oox\",\"Ộ\":\"Ooj\",\"Ơ\":\"Ow\",\n","\"Ằ\":\"AWF\",\"Ẳ\":\"Awr\",\"Ẵ\":\"Awx\",\"Ặ\":\"Awj\",\"Ó\":\"Os\",\"Ò\":\"Of\",\"Ỏ\":\"Or\",\"Õ\":\"Ox\",\"Ọ\":\"Oj\",\"Ô\":\"Oo\",\"Ố\":\"Oos\",\"Ồ\":\"Oof\",\n","\"Ớ\":\"Ows\",\"Ờ\":\"Owf\",\"Ở\":\"Owr\",\"Ỡ\":\"Owx\",\"Ợ\":\"Owj\",\"É\":\"Es\",\"È\":\"Ef\",\"Ẻ\":\"Er\",\"Ẽ\":\"Ex\",\"Ẹ\":\"Ej\",\"Ê\":\"Ee\",\"Ế\":\"Ees\",\"Ề\":\"Eef\",\n","\"Ể\":\"Eer\",\"Ễ\":\"Eex\",\"Ệ\":\"Eej\",\"Ú\":\"Us\",\"Ù\":\"Uf\",\"Ủ\":\"Ur\",\"Ũ\":\"Ux\",\"Ụ\":\"Uj\",\"Ư\":\"Uw\",\"Ứ\":\"Uws\",\"Ừ\":\"Uwf\",\"Ử\":\"Uwr\",\"Ữ\":\"Uwx\",\n","\"Ự\":\"Uwj\",\"Í\":\"Is\",\"Ì\":\"If\",\"Ỉ\":\"Ir\",\"Ị\":\"Ij\",\"Ĩ\":\"Ix\",\"Ý\":\"Ys\",\"Ỳ\":\"Yf\",\"Ỷ\":\"Yr\",\"Ỵ\":\"Yj\",\"Đ\":\"Dd\"}\n","\n","# dia phuong\n","region={\"ẻ\":\"ẽ\",\"ẽ\":\"ẻ\",\"ũ\":\"ủ\",\"ủ\":\"ũ\",\"ã\":\"ả\",\"ả\":\"ã\",\"ỏ\":\"õ\",\"õ\":\"ỏ\",\"i\":\"j\"}\n","region2={\"s\":\"x\",\"l\":\"n\",\"n\":\"l\",\"x\":\"s\",\"d\":\"gi\",\"S\":\"X\",\"L\":\"N\",\"N\":\"L\",\"X\":\"S\",\"Gi\":\"D\",\"D\":\"Gi\"}\n","\n","# nguyen am\n","vowel=list(\"aeiouyáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵ\")\n","\n","# viet tat\n","acronym={\"không\":\"ko\",\" anh\":\" a\",\"em\":\"e\",\"biết\":\"bít\",\"giờ\":\"h\",\"gì\":\"j\",\"muốn\":\"mún\",\"học\":\"hok\",\"yêu\":\"iu\",\n","         \"chồng\":\"ck\",\"vợ\":\"vk\",\" ông\":\" ô\",\"được\":\"đc\",\"tôi\":\"t\",\n","         \"Không\":\"Ko\",\" Anh\":\" A\",\"Em\":\"E\",\"Biết\":\"Bít\",\"Giờ\":\"H\",\"Gì\":\"J\",\"Muốn\":\"Mún\",\"Học\":\"Hok\",\"Yêu\":\"Iu\",\n","         \"Chồng\":\"Ck\",\"Vợ\":\"Vk\",\" Ông\":\" Ô\",\"Được\":\"Đc\",\"Tôi\":\"T\",}\n","\n","# teencode\n","teen={\"ch\":\"ck\",\"ph\":\"f\",\"th\":\"tk\",\"nh\":\"nk\",\n","      \"Ch\":\"Ck\",\"Ph\":\"F\",\"Th\":\"Tk\",\"Nh\":\"Nk\"}"]},{"cell_type":"markdown","metadata":{"id":"o78nlRaaCmu3"},"source":["### Hàm gây nhiễu, giả lập teencode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pob_Cy9oarAs"},"outputs":[],"source":["# function for adding mistake( noise)\n","def teen_code(sentence,pivot):\n","    random = np.random.uniform(0,1,1)[0]\n","    new_sentence=str(sentence)\n","    if random>pivot:\n","        for word in acronym.keys():\n","            if re.search(word, new_sentence):\n","                random2 = np.random.uniform(0,1,1)[0]\n","                if random2 <0.5:\n","                    new_sentence=new_sentence.replace(word,acronym[word])\n","        for word in teen.keys():\n","            if re.search(word, new_sentence):\n","                random3 = np.random.uniform(0,1,1)[0]\n","                if random3 <0.05:\n","                    new_sentence=new_sentence.replace(word,teen[word])\n","        return new_sentence\n","    else:\n","        return sentence\n","\n","\n","def add_noise(sentence, pivot1,pivot2):\n","    sentence=teen_code(sentence,0.5)\n","    noisy_sentence = \"\"\n","    i = 0\n","    while i < len(sentence):\n","        if sentence[i] not in letters:\n","            noisy_sentence+=sentence[i]\n","        else:\n","            random = np.random.uniform(0,1,1)[0]\n","            if random < pivot1:\n","                noisy_sentence+=(sentence[i])\n","            elif random<pivot2:\n","                if sentence[i] in typo.keys() and sentence[i] in region.keys():\n","                    random2=np.random.uniform(0,1,1)[0]\n","                    if random2<=0.4:\n","                        noisy_sentence+=typo[sentence[i]]\n","                    elif random2<0.8:\n","                        noisy_sentence+=region[sentence[i]]\n","                    elif random2<0.95 :\n","                        noisy_sentence+=unidecode(sentence[i])\n","                    else:\n","                        noisy_sentence+=sentence[i]\n","                elif sentence[i] in typo.keys():\n","                    random3=np.random.uniform(0,1,1)[0]\n","                    if random3<=0.6:\n","                        noisy_sentence+=typo[sentence[i]]\n","                    elif random3<0.9 :\n","                        noisy_sentence+=unidecode(sentence[i])\n","                    else:\n","                        noisy_sentence+=sentence[i]\n","                elif sentence[i] in region.keys():\n","                    random4=np.random.uniform(0,1,1)[0]\n","                    if random4<=0.6:\n","                        noisy_sentence+=region[sentence[i]]\n","                    elif random4<0.85 :\n","                        noisy_sentence+=unidecode(sentence[i])\n","                    else:\n","                        noisy_sentence+=sentence[i]\n","                elif i<len(sentence)-1 :\n","                    if sentence[i] in region2.keys() and (i==0 or sentence[i-1] not in letters) and sentence[i+1] in vowel:\n","                        random5=np.random.uniform(0,1,1)[0]\n","                        if random5<=0.9:\n","                            noisy_sentence+=region2[sentence[i]]\n","                        else:\n","                            noisy_sentence+=sentence[i]\n","                    else:\n","                        noisy_sentence+=sentence[i]\n","\n","            else:\n","                new_random = np.random.uniform(0,1,1)[0]\n","                if new_random <=0.33:\n","                    if i == (len(sentence) - 1):\n","                        continue\n","                    else:\n","                        noisy_sentence+=(sentence[i+1])\n","                        noisy_sentence+=(sentence[i])\n","                        i += 1\n","                elif new_random <= 0.66:\n","                    random_letter = np.random.choice(letters2, 1)[0]\n","                    noisy_sentence+=random_letter\n","                else:\n","                    pass\n","\n","        i += 1\n","    return noisy_sentence"]},{"cell_type":"markdown","source":["#### Thử nghiệm hàm add_noise lần lượt với các tham số của pivot1, pivot2"],"metadata":{"id":"pHt2g_i-INfJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQcnFITVCmu3","outputId":"1fae7563-c9bc-45a9-a8aa-d23b6218eeeb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Làm sạch văn bản, tách các cụm từ trong câu, nếu có kí tự đặc bjệt thì xóa chúng đi\n","Làm sach văn bản, tách các cụm tw trog câu, nế uc k ítự đgc biệt tì xóa chúng đi\n","Làm sich văn bản, tách các cụm ừt trno gnâu, nếp có kí to đặc biệt thì xóa chúng iđ\n"]}],"source":["print(add_noise('Làm sạch văn bản, tách các cụm từ trong câu, nếu có kí tự đặc biệt thì xóa chúng đi',0.94,0.985))\n","print(add_noise('Làm sạch văn bản, tách các cụm từ trong câu, nếu có kí tự đặc biệt thì xóa chúng đi',0.85,0.88))\n","print(add_noise('Làm sạch văn bản, tách các cụm từ trong câu, nếu có kí tự đặc biệt thì xóa chúng đi',0.8,0.9))"]},{"cell_type":"markdown","metadata":{"id":"QToUfBLCdRd8"},"source":["#### Làm sạch văn bản, tách các cụm từ trong câu, nếu có kí tự đặc biệt thì xóa chúng đi\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wb4qlCgYym0r"},"outputs":[],"source":["import itertools"]},{"cell_type":"markdown","source":["\\w: Đây là một ký tự thay thế có ý nghĩa trong regex, đại diện cho bất kỳ ký tự chữ cái (bao gồm cả chữ cái viết hoa và chữ cái viết thường) hoặc số. Nó không bao gồm ký tự không phải chữ cái hoặc số.\n","\n","[\\w ]+: Đoạn này tương ứng với một hoặc nhiều ký tự chữ cái (bao gồm cả dấu cách) hoặc số. Dấu cách ở đây cho phép xuất hiện trong cụm từ.\n","\n","r'...': Chuỗi được xác định bởi r trước dấu nháy đơn là một raw string, có nghĩa là các dấu gạch chéo không được xử lý đặc biệt. Điều này thường được sử dụng trong các biểu thức chính quy để tránh việc xử lý đặc biệt của các ký tự như \\.\n","\n","findall: Phương thức này của đối tượng regex được sử dụng để tìm tất cả các chuỗi con trong văn bản đầu vào mà khớp với biểu thức chính quy. Trong trường hợp này, nó trả về danh sách các cụm từ trong văn bản.\n","Hàm trả về một danh sách các cụm từ trong văn bản, trong đó mỗi cụm từ được định nghĩa bắt đầu từ một ký tự chữ cái hoặc số và có thể chứa cả dấu cách."],"metadata":{"id":"Pvgskl6gMn5b"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"g61rIjlnauFP"},"outputs":[],"source":["def extract_phrases(text):\n","    return re.findall(r'\\w[\\w ]+', text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cgvHdn9Na_7I"},"outputs":[],"source":["def _extract_phrases(data):\n","    phrases = itertools.chain.from_iterable(extract_phrases(text) for text in data)\n","    phrases = [p.strip() for p in phrases if len(p.split()) > 1]\n","\n","    return phrases"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXFni6LLy5W_","outputId":"4000972f-2de4-4bf7-89ce-7be767070710"},"outputs":[{"name":"stdout","output_type":"stream","text":["334297\n","2 triệu con gia cầm của 12 huyện thị\n"]}],"source":["phrases = _extract_phrases(training_data)\n","\n","print(len(phrases))\n","print(phrases[10])"]},{"cell_type":"markdown","metadata":{"id":"JXnuL9DJfPDs"},"source":["#### Chia tập ngữ điệu thành 2-gram. Từng phần tử trong list gồm có 2 từ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2m2uUHJQ6ZdT"},"outputs":[],"source":["from nltk import ngrams\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T11TNxY_6xde"},"outputs":[],"source":["# Một từ tiếng Việt không có quá 7 kí tự, một bigram lúc này không quá 15 kí tự\n","NGRAM = 2\n","MAXLEN = 40"]},{"cell_type":"markdown","source":["Với giá trị MAXLEN=40, hàm sẽ giữ lại các bi-grams có độ dài (tính bằng số ký tự) không vượt quá 40. Các bi-grams dài hơn 40 sẽ bị loại bỏ khỏi danh sách list gram."],"metadata":{"id":"eQSCOLtrPPMs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SyWAljsu2t7b"},"outputs":[],"source":["def gen_ngrams(words, n=2):\n","    return ngrams(words.split(), n)\n","\n","def generate_bi_grams(phrases):\n","    list_ngrams = []\n","    for p in tqdm(phrases):\n","\n","      # neu khong nam trong alphabet thi bo qua\n","      if not re.match(alphabet, p.lower()):\n","        continue\n","\n","      # tach p thanh cac bi gram\n","      for ngr in gen_ngrams(p, NGRAM):\n","        if len(\" \".join(ngr)) < MAXLEN:\n","          list_ngrams.append(\" \".join(ngr))\n","\n","    return list_ngrams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HwTIxtQs21Im","outputId":"1580bbbc-6b8f-4ad0-f86b-63e93c60d8b6"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/334297 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 334297/334297 [00:01<00:00, 218458.93it/s]\n"]}],"source":["list_ngrams = generate_bi_grams(phrases)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l7O_Ht4F7yum","outputId":"02e3e30f-7bd7-4577-cd1b-71d2512e7783"},"outputs":[{"name":"stdout","output_type":"stream","text":["2981627\n","triệu gia\n"]}],"source":["print(len(list_ngrams))\n","print(list_ngrams[7])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uX8jC5BRe8uy","outputId":"b28632be-93c5-4a10-997d-50f679ffcdfc"},"outputs":[{"name":"stdout","output_type":"stream","text":["['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n","199\n"]}],"source":["alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n","print(alphabet)\n","print(len(alphabet))"]},{"cell_type":"markdown","metadata":{"id":"CXt3D_tbisN2"},"source":["#3. Xây dựng mô hình"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43MbypBriuwJ","outputId":"6d6a566e-24c2-4e0c-8520-a124475a9bc1"},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-12-07 09:12:27.996293: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-12-07 09:12:27.998449: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-12-07 09:12:28.026942: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-12-07 09:12:28.027016: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-12-07 09:12:28.027043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-12-07 09:12:28.034726: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-12-07 09:12:28.035089: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-12-07 09:12:28.966837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["# Build the neural network\n","# this is adapted from the seq2seq architecture, which can be used for Machine Translation, Text Summarization Image Captioning ...\n","from keras.models import Sequential\n","from keras.layers import Activation, TimeDistributed, Dense,LSTM, Bidirectional\n","from keras.callbacks import Callback, ModelCheckpoint\n","from tensorflow.keras.optimizers import Adam # - Works"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-IeuGbS5vOl","outputId":"44ffea9d-4c6f-462f-f23d-ffce87d09ff7"},"outputs":[{"data":{"text/plain":["40"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["MAXLEN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URGzW2go5wTn","outputId":"9ba412a3-112c-400c-9ac2-0f573fad0cf6"},"outputs":[{"data":{"text/plain":["199"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["len(alphabet)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PPyg_Q2siyJr"},"outputs":[],"source":["encoder = LSTM(256,input_shape=(MAXLEN, len(alphabet)), return_sequences=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_zhW5WUyjEPn"},"outputs":[],"source":["decoder=Bidirectional(LSTM(256, return_sequences=True, dropout=0.2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zIrnqBxKjGU6"},"outputs":[],"source":["model=Sequential()\n","model.add(encoder)\n","model.add(decoder)\n","model.add(TimeDistributed(Dense(256)))\n","model.add(Activation('relu'))\n","model.add(TimeDistributed(Dense(len(alphabet))))\n","model.add(Activation('softmax'))"]},{"cell_type":"markdown","source":["Sử dụng mô hình Sequential.\\\n","là một lớp trong Keras cho phép bạn tạo ra một mô hình học máy tuần tự bằng cách thêm các layers một cách tuần tự.\\\n","tạo ra một layer LSTM với 256 units, sử dụng dữ liệu đầu vào có độ dài tối đa là MAXLEN và số lượng features tại mỗi time step là len(alphabet). \\\n","tạo ra một layer LSTM bidirectional với 256 units, và được áp dụng kỹ thuật dropout để kiểm soát quá mức và làm cho mô hình tổng quát hóa tốt hơn. \\\n","TimeDistributed(Dense(256)):\n","+ Dense(256): Là một tầng mạng nơ-ron đầy đủ kết nối (fully connected layer) với 256 đơn vị (neurons) hoặc nút. Tầng này kết nối mỗi đầu vào từ tầng trước nó với mỗi đầu ra của nó.\n","+ TimeDistributed(): Được sử dụng khi muốn áp dụng tầng mạng nơ-ron cho mỗi \"thời điểm\" của dữ liệu đầu vào.\\\n","\n","Activation('relu') là một hàm kích hoạt được sử dụng trong các mô hình mạng nơ-ron, và ở đây, 'relu' là viết tắt của Rectified Linear Unit. Hàm kích hoạt này thường được áp dụng sau một tầng mạng nơ-ron để giúp mô hình học được các đặc trưng phi tuyến tính và tăng cường khả năng học của nó.\n","+ Ưu điểm của ReLU: không có giới hạn dưới, điều này có thể giúp mô hình học được các đặc trưng phức tạp và biểu diễn chúng một cách hiệu quả. So với các hàm kích hoạt khác như sigmoid hoặc tanh, ReLU không đòi hỏi nhiều tính toán, điều này giúp tăng tốc quá trình huấn luyện mô hình.\n","+ Hạn chế: khi các đơn vị ReLU có thể trở thành 0 với đầu vào âm và không thể hồi phục lại, làm cho chúng không thể tham gia vào quá trình học.\n","\n","Activation('softmax'): Hàm softmax chuyển đầu ra của một tầng mạng nơ-ron thành một phân phối xác suất, tức là tạo ra một tập hợp các giá trị có tổng bằng 1, giúp mô hình có thể dự đoán lớp của mỗi mẫu dữ liệu.\n","\n","\n"],"metadata":{"id":"2_sfe9xrR20f"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2uL7KFl_jIgb","outputId":"56a21fac-7bba-48a7-ca3d-b0e4e4e0fff6"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (None, 40, 256)           466944    \n","                                                                 \n"," bidirectional (Bidirection  (None, 40, 512)           1050624   \n"," al)                                                             \n","                                                                 \n"," time_distributed (TimeDist  (None, 40, 256)           131328    \n"," ributed)                                                        \n","                                                                 \n"," activation (Activation)     (None, 40, 256)           0         \n","                                                                 \n"," time_distributed_1 (TimeDi  (None, 40, 199)           51143     \n"," stributed)                                                      \n","                                                                 \n"," activation_1 (Activation)   (None, 40, 199)           0         \n","                                                                 \n","=================================================================\n","Total params: 1700039 (6.49 MB)\n","Trainable params: 1700039 (6.49 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model.compile(loss='categorical_crossentropy',\n","              optimizer=Adam(lr=0.001),\n","              metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"markdown","source":["Chỉ số tốc độ học được sử dụng trong đề tài này là learning rate của hàm Adam, tham số này quyết định đến mức độ nhanh hay chậm của quá trình hội tụ. Nếu tốc độ học quá lớn khiến mô hình bỏ qua các điểm tối ưu cục bộ, dẫn đến kết quả cho ra kém. Nếu tốc độ học quá chậm dẫn đến mô hình hội tụ chậm, tốn thời gian đào tạo. Giá trị lr = 0.001 trong đề tài được đặt theo mặc định.\\\n","Tham số loss='categorical_crossentropy' được sử dụng để chỉ định hàm mất mát cho một mô hình phân loại đa lớp. Hàm mất mát này được tính bằng cách so sánh các xác suất dự đoán của mô hình với các nhãn thực tế."],"metadata":{"id":"OcCwhKCeV7kr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EoK3qdpujKqQ","outputId":"97d35674-51d5-4a53-fe9f-a202c041d80c"},"outputs":[{"name":"stdout","output_type":"stream","text":["You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"]}],"source":["from tensorflow.keras.utils import plot_model\n","plot_model(model, to_file='model.png')"]},{"cell_type":"markdown","source":["#### Tách data"],"metadata":{"id":"V1-uU8_8kHm5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5Wj2G2gjOak"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","train_data, valid_data = train_test_split(list_ngrams, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFoVo_yDjvIQ","outputId":"45b362a9-98da-4254-834f-429edcc2e891"},"outputs":[{"data":{"text/plain":["'triệu gia'"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["len(list_ngrams)\n","list_ngrams[7]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w94uZcE_jvwK","outputId":"af198be1-336e-4b21-f1f4-d8d8ddd0ea91"},"outputs":[{"name":"stdout","output_type":"stream","text":["2981627\n","2385301\n","596326\n","thuyền có\n"]}],"source":["print(len(list_ngrams))\n","print(len(train_data))\n","print(len(valid_data))\n","print(train_data[1])"]},{"cell_type":"markdown","metadata":{"id":"KtBfrCAjh8s8"},"source":["#### Mã hóa câu thành ma trận 2 chiều và giải mã câu đó"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aNZKniR1fhZj"},"outputs":[],"source":["# Padding hai đầu đoạn văn bản để tất cả bigram có độ dài bằng nhau\n","def encoder_data(text, maxlen=MAXLEN):\n","        #print(\"Maxlen\", maxlen)\n","        text = \"\\x00\" + text\n","        #print(\"text\", text)\n","        x = np.zeros((maxlen, len(alphabet)))\n","        #print(\"X ban dau\", x)\n","        for i, c in enumerate(text[:maxlen]):\n","            x[i, alphabet.index(c)] = 1\n","        if i < maxlen - 1:\n","          for j in range(i+1, maxlen):\n","            x[j, 0] = 1\n","        return x\n","\n","def decoder_data(x):\n","    x = x.argmax(axis=-1)\n","    #print(\"x hien tai\", x)\n","    dem = ''.join(alphabet[i] for i in x)\n","    #print(\"Do dai cau van\", len(dem))\n","\n","    return dem"]},{"cell_type":"markdown","source":["#### Qúa trình embedding một đoạn văn bản và trả về ma trận x của văn bản."],"metadata":{"id":"NHV39CAiQ1xc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LC_A8rc6gKwC","outputId":"46873515-78ed-4f88-a886-f968ca414dc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["(40, 199)\n","[[1. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]]\n"]}],"source":["print(encoder_data(\"Tôi tên là LÊ TUẤN\").shape)\n","print(encoder_data(\"Tôi tên là LÊ TUẤN\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzirJBY5kqpX"},"outputs":[],"source":["# Cần phải sử dụng data-generation vì dataset quá lớn để fit với memory\n","BATCH_SIZE = 512\n","def generate_data(data, batch_size):\n","    cur_index = 0\n","    while True:\n","        x, y = [], []\n","        for i in range(batch_size):\n","            y.append(encoder_data(data[cur_index]))\n","            x.append(encoder_data(add_noise(data[cur_index],0.94,0.985)))\n","            cur_index += 1\n","            if cur_index > len(data)-1:\n","                cur_index = 0\n","        yield np.array(x), np.array(y)"]},{"cell_type":"markdown","source":["BATCH_SIZE là kích thước của mỗi batch dữ liệu được sử dụng trong quá trình huấn luyện. Việc sử dụng generator và một kích thước batch nhỏ giúp đảm bảo tính linh hoạt và hiệu suất trong quá trình đào tạo mô hình."],"metadata":{"id":"4GTNvXfHgEao"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_t-VOdWk4PI"},"outputs":[],"source":["import os\n","train_generator = generate_data(train_data, batch_size=BATCH_SIZE)\n","validation_generator = generate_data(valid_data, batch_size=BATCH_SIZE)\n","#This is a Wikipedia-based image-text dataset for Vietnamese. It's extracted from Google WIT (https://github.com/google-research.../wit/blob/main/DATA.md). It contains raw images from Wikipedia (over 200 GBs). It can be used for image captioning, image-text retrieval, ...\n","#https://huggingface.co/datasets/dinhanhx/google-wit-vie=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HyIS2fFqCmu9","outputId":"01d8bec8-6d2c-4645-ca8f-db471a7c0803"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch X shape: [[[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," ...\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]]\n","Batch Y shape: [[[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," ...\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]]\n","Batch X shape: [[[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," ...\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]]\n","Batch Y shape: [[[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," ...\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]]\n","Batch X shape: [[[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," ...\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]]\n","Batch Y shape: [[[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," ...\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]]\n","Batch X shape: [[[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," ...\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]]\n","Batch Y shape: [[[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," ...\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]]\n","Batch X shape: [[[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," ...\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]]\n","Batch Y shape: [[[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," ...\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]\n","\n"," [[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 0. ... 0. 0. 0.]\n","  ...\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]\n","  [1. 0. 0. ... 0. 0. 0.]]]\n"]}],"source":["for i in range(5):  # Generate 5 batches for demonstration\n","    batch_x, batch_y = next(train_generator)\n","    print(\"Batch X shape:\", batch_x)\n","    print(\"Batch Y shape:\", batch_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-9-dLlLk6Ca"},"outputs":[],"source":["# train the model and save to the Model folder\n","checkpointer = ModelCheckpoint(filepath='/home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5', save_best_only=True, verbose=1)"]},{"cell_type":"markdown","source":["train_generator: Đối tượng generator cho dữ liệu huấn luyện. Generator là một cách hiệu quả để xử lý và đưa dữ liệu vào mô hình dần dần thay vì nạp toàn bộ dữ liệu vào bộ nhớ.\\\n","steps_per_epoch: Số bước (steps) mỗi epoch. Một bước là một lần cập nhật trọng số của mô hình dựa trên một lô dữ liệu. len(train_data)//BATCH_SIZE tính toán số bước cần thiết để duyệt qua toàn bộ dữ liệu đào tạo.\\\n","epochs: Số lần duyệt qua toàn bộ dữ liệu đào tạo. Ở đây, mô hình sẽ được đào tạo qua dữ liệu đào tạo 5 lần.\\\n","validation_data: Đối tượng generator cho dữ liệu validation. Dữ liệu này được sử dụng để đánh giá hiệu suất của mô hình sau mỗi epoch.\\\n","validation_steps: Số bước (steps) cho dữ liệu validation. Tương tự như steps_per_epoch, được tính toán từ len(valid_data)//BATCH_SIZE.\\\n","callbacks: Danh sách các callback được sử dụng trong quá trình huấn luyện. Ở đây, checkpointer có thể là một callback để lưu trọng số của mô hình khi có sự cải thiện trên dữ liệu validation.\\"],"metadata":{"id":"qwR7uOrTh50J"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkqPYVcGlc1z","outputId":"fb43616d-3842-4c19-f5a2-73d36f60ddc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","4658/4658 [==============================] - ETA: 0s - loss: 0.1030 - accuracy: 0.9782\n","Epoch 1: val_loss improved from inf to 0.02529, saving model to /home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5\n","4658/4658 [==============================] - 5512s 1s/step - loss: 0.1030 - accuracy: 0.9782 - val_loss: 0.0253 - val_accuracy: 0.9940\n","Epoch 2/5\n"]},{"name":"stderr","output_type":"stream","text":["/home/phuong/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"name":"stdout","output_type":"stream","text":["4658/4658 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9946\n","Epoch 2: val_loss improved from 0.02529 to 0.01831, saving model to /home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5\n","4658/4658 [==============================] - 6245s 1s/step - loss: 0.0225 - accuracy: 0.9946 - val_loss: 0.0183 - val_accuracy: 0.9955\n","Epoch 3/5\n","4658/4658 [==============================] - ETA: 0s - loss: 0.0176 - accuracy: 0.9956\n","Epoch 3: val_loss improved from 0.01831 to 0.01600, saving model to /home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5\n","4658/4658 [==============================] - 6282s 1s/step - loss: 0.0176 - accuracy: 0.9956 - val_loss: 0.0160 - val_accuracy: 0.9960\n","Epoch 4/5\n","4658/4658 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9961\n","Epoch 4: val_loss improved from 0.01600 to 0.01427, saving model to /home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5\n","4658/4658 [==============================] - 6273s 1s/step - loss: 0.0155 - accuracy: 0.9961 - val_loss: 0.0143 - val_accuracy: 0.9963\n","Epoch 5/5\n","4658/4658 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9963\n","Epoch 5: val_loss improved from 0.01427 to 0.01307, saving model to /home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5\n","4658/4658 [==============================] - 6266s 1s/step - loss: 0.0143 - accuracy: 0.9963 - val_loss: 0.0131 - val_accuracy: 0.9966\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x7ff9540a6010>"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["model.fit( train_generator, steps_per_epoch=len(train_data)//BATCH_SIZE, epochs=5,\n","                    validation_data=validation_generator, validation_steps=len(valid_data)//BATCH_SIZE,\n","                    callbacks=[checkpointer] )"]},{"cell_type":"markdown","metadata":{"id":"tRewGYVd-GIJ"},"source":["#5.DỰ ĐOÁN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LOYH9uWF-OAf"},"outputs":[],"source":["from collections import Counter\n","from keras.models import load_model\n","from nltk.tokenize import word_tokenize\n","from nltk import ngrams,word_tokenize\n","import numpy as np\n","import re\n","import unidecode\n","import string\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zajwNyjRAVtK","outputId":"d13e19fc-4a7f-4200-d6fc-d77fdf13cd96"},"outputs":[{"data":{"text/plain":["<tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x7ff8f4dc2e50>"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["#model = load_model('E:\\Python\\language\\spell_0.99.h5')\n","model = load_model('/home/phuong/Documents/2023_NLP/Code/Model/my_model_weight.h5')\n","model.make_predict_function()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-XjsEp02AduP"},"outputs":[],"source":["NGRAM=2\n","MAXLEN=40 # INPUT SHAPE\n","alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n","letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n","accepted_char=list((string.digits + ''.join(letters)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JCLDKMzPl19O"},"outputs":[],"source":["def call(sentence):\n","    def extract_phrases(text):\n","        pattern = r'\\w[\\w ]*|\\s\\W+|\\W+'\n","        return re.findall(pattern, text)\n","\n","    def encoder_data(text, maxlen=MAXLEN):\n","            text = \"\\x00\" + text\n","            x = np.zeros((maxlen, len(alphabet)))\n","            for i, c in enumerate(text[:maxlen]):\n","                x[i, alphabet.index(c)] = 1\n","            if i < maxlen - 1:\n","              for j in range(i+1, maxlen):\n","                x[j, 0] = 1\n","            return x\n","\n","    def decoder_data(x):\n","        x = x.argmax(axis=-1)\n","        return ''.join(alphabet[i] for i in x)\n","\n","    def nltk_ngrams(words, n=2):\n","        return ngrams(words.split(), n)\n","\n","    def guess(ngram):\n","        text = ' '.join(ngram)\n","        preds = model.predict(np.array([encoder_data(text)]), verbose=0)\n","        return decoder_data(preds[0]).strip('\\x00')\n","\n","    def correct(sentence):\n","        for i in sentence:\n","            if i not in accepted_char:\n","                sentence=sentence.replace(i,\" \")\n","        ngrams = list(nltk_ngrams(sentence, n=NGRAM))\n","        guessed_ngrams = list(guess(ngram) for ngram in ngrams)\n","\n","        print(\"N gram\", ngrams)\n","        print(\"guess\", guessed_ngrams)\n","\n","        #return guessed_ngrams\n","\n","        candidates = [Counter() for _ in range(len(guessed_ngrams) + NGRAM - 1)]\n","        for nid, ngram in (enumerate(guessed_ngrams)):\n","            for wid, word in (enumerate(re.split(' +', ngram))):\n","                candidates[nid + wid].update([word])\n","\n","        # for c in candidates:\n","        #     print(c.most_common(1))\n","        output = ' '.join(c.most_common(1)[0][0] for c in candidates)\n","        return output\n","\n","    guess = correct(sentence)\n","\n","    return guess"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEibQsCG-aLS","outputId":"a2b4b120-8e4d-4264-a76f-de164b438ed2"},"outputs":[{"name":"stdout","output_type":"stream","text":["N gram [('Các', 'phát'), ('phát', 'thank'), ('thank', 'viên'), ('viên', 'ngưowfi'), ('ngưowfi', 'dẫn'), ('dẫn', 'chương'), ('chương', 'trìnk'), ('trìnk', 'đeu'), ('đeu', 'là'), ('là', 'cac'), ('cac', 'chuyeen'), ('chuyeen', 'gja'), ('gja', 'sử'), ('sử', 'dụng'), ('dụng', 'ngôn'), ('ngôn', 'ngữ'), ('ngữ', 'Thế'), ('Thế', 'nkưng'), ('nkưng', 'đa'), ('đa', 'phần'), ('phần', 'họ'), ('họ', 'ềđu'), ('ềđu', 'cho'), ('cho', 'rằng'), ('rằng', 'mìnk'), ('mìnk', 'ko'), ('ko', 'có'), ('có', 'khiếu'), ('khiếu', 'ăn'), ('ăn', 'nosi'), ('nosi', 'từ'), ('từ', 'nkỏ'), ('nkỏ', 'vậy'), ('vậy', 'tạo'), ('tạo', 'sao'), ('sao', 'hoj'), ('hoj', 'vẫn'), ('vẫn', 'thafnh'), ('thafnh', 'công'), ('công', 'nkờ'), ('nkờ', 'vào'), ('vào', 'tài'), ('tài', 'ăn'), ('ăn', 'nói'), ('nói', 'của'), ('của', 'mìnk'), ('mìnk', 'Nguyên'), ('Nguyên', 'nkân'), ('nkân', 'rất'), ('rất', 'đơn'), ('đơn', 'giản'), ('giản', 'đó'), ('đó', 'là'), ('là', 'vì'), ('vì', 'họ'), ('họ', 'tự'), ('tự', 'nkận'), ('nkận', 'thấy'), ('thấy', 'mìnk'), ('mìnk', 'nói'), ('nói', 'năng'), ('năng', 'ko'), ('ko', 'tốt'), ('tốt', 'nên'), ('nên', 'luôn'), ('luôn', 'cố'), ('cố', 'gắng'), ('gắng', 'để'), ('để', 'nâng'), ('nâng', 'cao'), ('cao', 'kĩ'), ('kĩ', 'năng'), ('năng', 'giao'), ('giao', 'tiế')]\n","guess ['Các phát', 'phát thanh', 'thanh viên', 'viên người', 'người dẫn', 'dẫn chương', 'chương trình', 'trình đều', 'đều là', 'là các', 'các chuyên', 'chuyên gia', 'gia sử', 'sử dụng', 'dụng ngôn', 'ngôn ngữ', 'ngữ Thế', 'Thế nhưng', 'nhưng đa', 'đa phần', 'phần họ', 'họ đều', 'đều cho', 'cho rằng', 'rằng mình', 'mình không', 'không có', 'có khiếu', 'khiếu ăn', 'ăn nói', 'nói từ', 'từ nhỏ', 'nhỏ vậy', 'vậy tạo', 'tạo sao', 'sao học', 'họ vẫn', 'vẫn thành', 'thành công', 'công nhờ', 'nhờ vào', 'vào tài', 'tài ăn', 'ăn nói', 'nói của', 'của mình', 'mình Nguyên', 'Nguyên nhân', 'nhân rất', 'rất đơn', 'đơn giản', 'giản đó', 'đó là', 'là vì', 'vì họ', 'họ tự', 'tự nhận', 'nhận thấy', 'thấy mình', 'mình nói', 'nói năng', 'năng không', 'không tốt', 'tốt nên', 'nên luôn', 'luôn cố', 'cố gắng', 'gắng để', 'để nâng', 'nâng cao', 'cao kĩ', 'kĩ năng', 'năng giao', 'giao tiếp']\n","Các phát thanh viên người dẫn chương trình đều là các chuyên gia sử dụng ngôn ngữ Thế nhưng đa phần họ đều cho rằng mình không có khiếu ăn nói từ nhỏ vậy tạo sao học vẫn thành công nhờ vào tài ăn nói của mình Nguyên nhân rất đơn giản đó là vì họ tự nhận thấy mình nói năng không tốt nên luôn cố gắng để nâng cao kĩ năng giao tiếp\n"]}],"source":["#sentence = \"Các phát thank viên dẫn cương trink\"\n","#sentence = \"Hom lay tooi đi hoc\"\n","#sentence ='Tôi yêu thích họk ngôn ngữ mớj'\n","#sentence = 'Thnh pố Hồ Chí Mình là mộp địa điểm du lịch nổi tiếg'\n","sentence = 'Các phát thank viên, ngưowfi dẫn chương trìnk đeu là cac chuyeen gja sử dụng ngôn ngữ. Thế nkưng đa phần họ ềđu cho rằng mìnk ko có khiếu ăn nosi từ nkỏ, vậy tạo sao hoj vẫn thafnh công nkờ vào tài ăn nói của mìnk? Nguyên nkân rất đơn giản, đó là vì họ tự nkận thấy mìnk nói năng ko tốt, nên luôn cố gắng để nâng cao kĩ năng giao tiế'\n","guess = call(sentence)\n","print(guess)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JodFSbMjCmu-"},"outputs":[],"source":["import pickle\n","with open('/home/phuong/Documents/2023_NLP/Code/PreData/test_corpus.pkl', \"rb\") as f:\n","    data = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eD_HSrcQCmu-","outputId":"8b843e0b-541e-4d2e-96c9-d7ea05e05be2"},"outputs":[{"data":{"text/plain":["8994"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["len(data)"]}],"metadata":{"colab":{"collapsed_sections":["gV7gnnYnZMns","GMAmC-ktKreu","hEXUdGdxKxw3","CXt3D_tbisN2","g4oXdRcLuEgw"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}